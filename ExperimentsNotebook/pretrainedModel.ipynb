{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoProcessor\n",
    "\n",
    "# Load the processor manually\n",
    "processor = AutoProcessor.from_pretrained(\"prathameshdalal/vivit-b-16x2-kinetics400-UCF-Crime\")\n",
    "\n",
    "# Initialize the pipeline and pass the processor explicitly\n",
    "video_classification_pipeline = pipeline(\n",
    "    \"video-classification\",\n",
    "    model=\"prathameshdalal/vivit-b-16x2-kinetics400-UCF-Crime\",\n",
    "    image_processor=processor  # Explicitly pass the processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video successfully converted to grayscale and saved as: /Users/prabeshsharma/Documents/Unsual_activity_Detection/grayscale_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def convert_video_to_grayscale(input_video_path, output_video_path):\n",
    "    # Open the input video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    # Get the width, height, and frames per second (fps) of the input video\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # Define the codec and create VideoWriter object to save the output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4 video\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height), isColor=False)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert frame to grayscale\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Write the grayscale frame to the output video\n",
    "        out.write(gray_frame)\n",
    "    \n",
    "    # Release everything\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "# Specify input and output paths\n",
    "input_video_path = \"/Users/prabeshsharma/Documents/Unsual_activity_Detection/WhatsApp Video 2024-10-19 at 11.16.43.mp4\"\n",
    "output_video_path = \"/Users/prabeshsharma/Documents/Unsual_activity_Detection/grayscale_video.mp4\"\n",
    "\n",
    "# Convert video to grayscale\n",
    "convert_video_to_grayscale(input_video_path, output_video_path)\n",
    "\n",
    "print(\"Video successfully converted to grayscale and saved as:\", output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.9721571803092957, 'label': 'Normal'}, {'score': 0.0074067856185138226, 'label': 'RoadAccidents'}, {'score': 0.006915787234902382, 'label': 'Arrest'}, {'score': 0.0023876947816461325, 'label': 'Robbery'}, {'score': 0.0019720157142728567, 'label': 'Shooting'}]\n"
     ]
    }
   ],
   "source": [
    "video_path = '/Users/prabeshsharma/Documents/Unsual_activity_Detection/grayscale_video.mp4'\n",
    "results = video_classification_pipeline(video_path)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88fe405408f3496aaa048cf98d3987ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/415 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b456349727e84df9a43ea208eb80408c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/961 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34121dde341946559496f7eacb0e76f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/345M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoProcessor\n",
    "\n",
    "# Load the processor manually\n",
    "processor = AutoProcessor.from_pretrained(\"Prabesh06/videomae-base-finetuned-ucf101-normalAbnormal\")\n",
    "\n",
    "# Initialize the pipeline and pass the processor explicitly\n",
    "video_classification_pipeline = pipeline(\n",
    "    \"video-classification\",\n",
    "    model=\"Prabesh06/videomae-base-finetuned-ucf101-normalAbnormal\",\n",
    "    image_processor=processor  # Explicitly pass the processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video successfully converted to grayscale and saved as: /Users/prabeshsharma/Documents/Unsual_activity_Detection/grayscale_video.avi\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def convert_video_to_grayscale(input_video_path, output_video_path):\n",
    "    # Open the input video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    # Get the width, height, and frames per second (fps) of the input video\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # Define the codec and create VideoWriter object to save the output video in AVI format\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec for AVI format\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height), isColor=False)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert frame to grayscale\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Write the grayscale frame to the output video\n",
    "        out.write(gray_frame)\n",
    "    \n",
    "    # Release everything\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "# Specify input and output paths\n",
    "input_video_path = \"/Users/prabeshsharma/Documents/Unsual_activity_Detection/2024-10-03 13.46.47.mp4\"\n",
    "output_video_path = \"/Users/prabeshsharma/Documents/Unsual_activity_Detection/grayscale_video.avi\"\n",
    "\n",
    "# Convert video to grayscale and save as AVI\n",
    "convert_video_to_grayscale(input_video_path, output_video_path)\n",
    "\n",
    "print(\"Video successfully converted to grayscale and saved as:\", output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.7946946024894714, 'label': 'AbnormalVideos'}, {'score': 0.20530538260936737, 'label': 'NormalVideos'}]\n"
     ]
    }
   ],
   "source": [
    "video_path = '/Users/prabeshsharma/Documents/Unsual_activity_Detection/Shooting023_x264.mp4'\n",
    "results = video_classification_pipeline(video_path)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Model with Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoImageProcessor, AutoModelForVideoClassification\n",
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "\n",
    "\n",
    "model = AutoModelForVideoClassification.from_pretrained(\"Prabesh06/videomae-base-finetuned-ucf101-normalAbnormal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63617eda3de2438c88220a311e025e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_processor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.16.1 (from versions: 0.17.0, 0.17.1, 0.17.2, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.20.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision==0.16.1\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install torchvision==0.16.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision.transforms.functional_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     ApplyTransformToKey,\n\u001b[1;32m      5\u001b[0m     Normalize,\n\u001b[1;32m      6\u001b[0m     RandomShortSideScale,\n\u001b[1;32m      7\u001b[0m     RemoveKey,\n\u001b[1;32m      8\u001b[0m     ShortSideScale,\n\u001b[1;32m      9\u001b[0m     UniformTemporalSubsample,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     Compose,\n\u001b[1;32m     14\u001b[0m     Lambda,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     Resize,\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorchvideo/transforms/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AugMix  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CutMix, MixUp, MixVideo  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrand_augment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandAugment  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorchvideo/transforms/augmix.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Optional\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmentations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     _AUGMENTATION_MAX_LEVEL,\n\u001b[1;32m      8\u001b[0m     AugmentTransform,\n\u001b[1;32m      9\u001b[0m     _decreasing_int_to_arg,\n\u001b[1;32m     10\u001b[0m     _decreasing_to_arg,\n\u001b[1;32m     11\u001b[0m     _increasing_magnitude_to_arg,\n\u001b[1;32m     12\u001b[0m     _increasing_randomly_negate_to_arg,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpSampler\n\u001b[1;32m     17\u001b[0m _AUGMIX_LEVEL_TO_ARG \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoContrast\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEqualize\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdjustSharpness\u001b[39m\u001b[38;5;124m\"\u001b[39m: _increasing_magnitude_to_arg,\n\u001b[1;32m     31\u001b[0m }\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorchvideo/transforms/augmentations.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional_tensor\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF_t\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Maximum global magnitude used for video augmentation.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision.transforms.functional_tensor'"
     ]
    }
   ],
   "source": [
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision.transforms.functional_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Compose, Lambda, Resize\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UniformTemporalSubsample\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoImageProcessor, AutoModelForVideoClassification\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorchvideo/transforms/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AugMix  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CutMix, MixUp, MixVideo  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrand_augment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandAugment  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorchvideo/transforms/augmix.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Optional\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmentations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     _AUGMENTATION_MAX_LEVEL,\n\u001b[1;32m      8\u001b[0m     AugmentTransform,\n\u001b[1;32m      9\u001b[0m     _decreasing_int_to_arg,\n\u001b[1;32m     10\u001b[0m     _decreasing_to_arg,\n\u001b[1;32m     11\u001b[0m     _increasing_magnitude_to_arg,\n\u001b[1;32m     12\u001b[0m     _increasing_randomly_negate_to_arg,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpSampler\n\u001b[1;32m     17\u001b[0m _AUGMIX_LEVEL_TO_ARG \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoContrast\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEqualize\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdjustSharpness\u001b[39m\u001b[38;5;124m\"\u001b[39m: _increasing_magnitude_to_arg,\n\u001b[1;32m     31\u001b[0m }\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorchvideo/transforms/augmentations.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional_tensor\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF_t\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Maximum global magnitude used for video augmentation.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision.transforms.functional_tensor'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Lambda, Resize\n",
    "from pytorchvideo.transforms import UniformTemporalSubsample\n",
    "from transformers import AutoImageProcessor, AutoModelForVideoClassification\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Compose, Lambda, Resize, Normalize\n",
    "from transformers import AutoImageProcessor, AutoModelForVideoClassification\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load model and processor\n",
    "model = AutoModelForVideoClassification.from_pretrained(\"Prabesh06/videomae-base-finetuned-ucf101-normalAbnormal\")\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "\n",
    "# Set parameters\n",
    "mean = torch.tensor(image_processor.image_mean).view(3, 1, 1)  # For normalization\n",
    "std = torch.tensor(image_processor.image_std).view(3, 1, 1)\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "sample_rate = 4\n",
    "height, width = 224, 224  # Example size, update to match your input requirements\n",
    "\n",
    "# Function to subsample frames manually\n",
    "def subsample_frames(frames, num_samples):\n",
    "    indices = np.linspace(0, len(frames) - 1, num_samples, dtype=int)\n",
    "    return [frames[i] for i in indices]\n",
    "\n",
    "# Function to preprocess video\n",
    "def preprocess_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    # Capture frames from the video\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "    # Subsample frames\n",
    "    frames = subsample_frames(frames, num_frames_to_sample)\n",
    "\n",
    "    # Convert frames to tensor\n",
    "    frames_tensor = torch.tensor(frames).permute(0, 3, 1, 2).float()  # (T, C, H, W)\n",
    "    \n",
    "    # Define transformations\n",
    "    transform = Compose([\n",
    "        Lambda(lambda x: x / 255.0),  # Scale pixel values to [0, 1]\n",
    "        Resize((height, width)),\n",
    "        Lambda(lambda x: (x - mean) / std),  # Normalize\n",
    "    ])\n",
    "    \n",
    "    # Apply transformations to each frame\n",
    "    frames_tensor = torch.stack([transform(frame) for frame in frames_tensor])\n",
    "\n",
    "    # Add batch dimension\n",
    "    frames_tensor = frames_tensor.unsqueeze(0)\n",
    "    \n",
    "    return frames_tensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.1891915500164032, 'label': 'AbnormalVideos'}, {'score': 0.8108084201812744, 'label': 'NormalVideos'}]\n"
     ]
    }
   ],
   "source": [
    "# Classify the video\n",
    "video_path = \"/Users/prabeshsharma/Documents/Unsual_activity_Detection/2024-10-03 13.46.47.mp4\"\n",
    "input_frames = preprocess_video(video_path)\n",
    "outputs = model(input_frames)\n",
    "\n",
    "# Convert logits to probabilities and format output\n",
    "probabilities = torch.softmax(outputs.logits, dim=-1)\n",
    "class_labels = [\"AbnormalVideos\", \"NormalVideos\"]\n",
    "output = [{\"score\": prob.item(), \"label\": label} for prob, label in zip(probabilities[0], class_labels)]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
